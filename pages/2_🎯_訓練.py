"""
è¨“ç·´é é¢

æä¾›æ¨¡å‹è¨“ç·´åŠŸèƒ½ï¼Œæ”¯æ´ç·šæ€§å›æ­¸å’Œæ¢¯åº¦ä¸‹é™è¨“ç·´ã€‚
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from utils.data_loader import load_csv_file, validate_dataframe
from utils.model_manager import save_model, list_models
from utils.data_preprocessor import (
    detect_categorical_features, 
    convert_percentage_columns,
    convert_thousand_separator_columns
)
from utils.visualizer import (
    plot_training_loss,
    display_model_parameters,
    display_model_parameters_with_pvalues,
    display_evaluation_metrics
)
from models.linear_regression import LinearRegressionModel
from models.gradient_descent import GradientDescentModel


st.title("ğŸ¯ æ¨¡å‹è¨“ç·´")
st.markdown("---")

# èªªæ˜æ–‡å­—
st.info(
    "ğŸ‘‹ æ­¡è¿ä½¿ç”¨æ¨¡å‹è¨“ç·´åŠŸèƒ½ï¼\n\n"
    "è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿè¨“ç·´æ¨¡å‹ï¼š\n"
    "1. é¸æ“‡æˆ–ä¸Šå‚³è¨“ç·´è³‡æ–™\n"
    "2. é¸æ“‡ç›®æ¨™è®Šæ•¸ï¼ˆè¦é æ¸¬çš„æ¬„ä½ï¼‰\n"
    "3. é¸æ“‡è¨“ç·´æ¼”ç®—æ³•å’Œåƒæ•¸\n"
    "4. é–‹å§‹è¨“ç·´ä¸¦æŸ¥çœ‹çµæœ\n"
    "5. ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹"
)

# ========== æ­¥é©Ÿ 1: è³‡æ–™é¸æ“‡ ==========
st.markdown("### æ­¥é©Ÿ 1: é¸æ“‡è¨“ç·´è³‡æ–™")

# å„ªå…ˆæª¢æŸ¥ session state ä¸­æ˜¯å¦æœ‰è³‡æ–™åˆ†æé é¢çš„è³‡æ–™
df = None
data_source = "session"

if 'data_analysis_df' in st.session_state and st.session_state['data_analysis_df'] is not None:
    df = st.session_state['data_analysis_df']
    st.success(f"âœ… å·²å¾è³‡æ–™åˆ†æé é¢è¼‰å…¥è³‡æ–™ï¼ˆ{df.shape[0]} ç­†ï¼Œ{df.shape[1]} æ¬„ä½ï¼‰")
    st.dataframe(df.head(5), use_container_width=True)
    
    use_existing = st.checkbox("ä½¿ç”¨æ­¤è³‡æ–™", value=True)
    if not use_existing:
        df = None
        data_source = "upload"

if df is None:
    data_source = "upload"
    uploaded_file = st.file_uploader(
        "æˆ–ä¸Šå‚³æ–°çš„ CSV æª”æ¡ˆ",
        type=['csv'],
        help="è«‹é¸æ“‡ä¸€å€‹ CSV æ ¼å¼çš„è³‡æ–™æª”æ¡ˆ"
    )
    
    if uploaded_file is not None:
        with st.spinner("æ­£åœ¨è¼‰å…¥è³‡æ–™..."):
            df, error_message = load_csv_file(uploaded_file)
            
            if error_message:
                st.error(f"âŒ {error_message}")
                st.stop()
            
            is_valid, validation_error = validate_dataframe(df)
            
            if not is_valid:
                st.error(f"âŒ {validation_error}")
                st.stop()
            
            st.success("âœ… è³‡æ–™è¼‰å…¥æˆåŠŸï¼")
            st.dataframe(df.head(5), use_container_width=True)

if df is None:
    st.warning("ğŸ‘† è«‹å…ˆé¸æ“‡æˆ–ä¸Šå‚³è³‡æ–™")
    st.stop()

st.markdown("---")

# ========== æ­¥é©Ÿ 2: æ’é™¤æ¬„ä½é¸æ“‡ ==========
st.markdown("### æ­¥é©Ÿ 2: é¸æ“‡è¦æ’é™¤çš„æ¬„ä½ï¼ˆé¸å¡«ï¼‰")

# å–å¾—æ‰€æœ‰æ¬„ä½ï¼ˆåŒ…æ‹¬éæ•¸å€¼æ¬„ä½ï¼‰
all_columns = df.columns.tolist()

# åˆå§‹åŒ–æ’é™¤æ¬„ä½åˆ—è¡¨
excluded_columns = []

if all_columns:
    excluded_columns = st.multiselect(
        "é¸æ“‡è¦æ’é™¤çš„æ¬„ä½ï¼ˆä¸æœƒç”¨æ–¼è¨“ç·´ï¼‰",
        options=all_columns,
        help="å¯ä»¥é¸æ“‡ä¸€å€‹æˆ–å¤šå€‹ä¸éœ€è¦ç”¨æ–¼è¨“ç·´çš„æ¬„ä½ï¼Œä¾‹å¦‚ IDã€ç´¢å¼•ã€æ—¥æœŸç­‰ã€‚é€™äº›æ¬„ä½å°‡ä¸æœƒä½œç‚ºç‰¹å¾µè®Šæ•¸æˆ–ç›®æ¨™è®Šæ•¸ã€‚"
    )
    
    if excluded_columns:
        # é¡¯ç¤ºæ’é™¤çš„æ¬„ä½è³‡è¨Š
        excluded_numeric = [col for col in excluded_columns if col in df.select_dtypes(include=[np.number]).columns.tolist()]
        excluded_non_numeric = [col for col in excluded_columns if col not in excluded_numeric]
        
        info_parts = []
        if excluded_numeric:
            info_parts.append(f"{len(excluded_numeric)} å€‹æ•¸å€¼æ¬„ä½")
        if excluded_non_numeric:
            info_parts.append(f"{len(excluded_non_numeric)} å€‹éæ•¸å€¼æ¬„ä½")
        
        st.info(f"âœ… å·²æ’é™¤ {len(excluded_columns)} å€‹æ¬„ä½ï¼ˆ{', '.join(info_parts)}ï¼‰ï¼š{', '.join(excluded_columns)}")
else:
    st.info("â„¹ï¸ æ²’æœ‰å¯æ’é™¤çš„æ¬„ä½")

st.markdown("---")

# ========== æ­¥é©Ÿ 3: è³‡æ–™é è™•ç†è¨­å®š ==========
st.markdown("### æ­¥é©Ÿ 3: è³‡æ–™é è™•ç†è¨­å®š")

# å–å¾—å¯ç”¨æ¬„ä½ï¼ˆæ’é™¤å·²æ’é™¤çš„æ¬„ä½ï¼‰
available_columns = [col for col in df.columns if col not in excluded_columns]

if not available_columns:
    st.error("âŒ æ²’æœ‰å¯ç”¨çš„æ¬„ä½ï¼ˆæ‰€æœ‰æ¬„ä½éƒ½å·²è¢«æ’é™¤ï¼‰ã€‚")
    st.stop()

# å°æ‰€æœ‰å¯ç”¨æ¬„ä½é€²è¡Œé è™•ç†é è¦½
df_preview = df[available_columns].copy()

# å…ˆè½‰æ›åƒåˆ†ä½åˆ†éš”ç¬¦æ¬„ä½ç‚ºæ•¸å€¼
df_preprocessed, thousand_separator_columns_detected = convert_thousand_separator_columns(df_preview)

# å†è½‰æ›ç™¾åˆ†æ¯”æ¬„ä½ç‚ºæ•¸å€¼
df_preprocessed, percentage_columns_detected = convert_percentage_columns(df_preprocessed)

# æª¢æ¸¬é¡åˆ¥å‹ç‰¹å¾µï¼ˆåœ¨è½‰æ›åƒåˆ†ä½å’Œç™¾åˆ†æ¯”ä¹‹å¾Œï¼‰
categorical_features_detected = detect_categorical_features(df_preprocessed)

# é¡¯ç¤ºåƒåˆ†ä½åˆ†éš”ç¬¦æ¬„ä½è³‡è¨Š
if thousand_separator_columns_detected:
    st.success(
        f"ğŸ”¢ æª¢æ¸¬åˆ° {len(thousand_separator_columns_detected)} å€‹å¸¶æœ‰åƒåˆ†ä½åˆ†éš”ç¬¦çš„æ¬„ä½ï¼š\n\n"
        f"**{', '.join(thousand_separator_columns_detected)}**\n\n"
        "âœ… é€™äº›æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼æ ¼å¼ï¼ˆä¾‹å¦‚ \"1,000\" â†’ 1000ï¼‰ã€‚"
    )

# é¡¯ç¤ºç™¾åˆ†æ¯”æ¬„ä½è³‡è¨Š
if percentage_columns_detected:
    st.success(
        f"ğŸ“Š æª¢æ¸¬åˆ° {len(percentage_columns_detected)} å€‹ç™¾åˆ†æ¯”æ¬„ä½ï¼š\n\n"
        f"**{', '.join(percentage_columns_detected)}**\n\n"
        "âœ… é€™äº›æ¬„ä½å°‡è‡ªå‹•è½‰æ›ç‚ºæ•¸å€¼æ ¼å¼ï¼ˆä¾‹å¦‚ \"50%\" â†’ 0.5ï¼‰ã€‚"
    )

if categorical_features_detected:
    st.success(
        f"ğŸ” æª¢æ¸¬åˆ° {len(categorical_features_detected)} å€‹é¡åˆ¥å‹ç‰¹å¾µï¼š\n\n"
        f"**{', '.join(categorical_features_detected)}**\n\n"
        "âœ… é€™äº›ç‰¹å¾µå°‡è‡ªå‹•ä½¿ç”¨**ç¨ç†±ç·¨ç¢¼ï¼ˆOne-Hot Encodingï¼‰**è™•ç†ã€‚"
    )
    
    # é¡¯ç¤ºé¡åˆ¥å‹ç‰¹å¾µçš„è©³ç´°è³‡è¨Š
    with st.expander("ğŸ“‹ æŸ¥çœ‹é¡åˆ¥å‹ç‰¹å¾µè©³ç´°è³‡è¨Š"):
        for cat_feat in categorical_features_detected:
            unique_values = df_preview[cat_feat].unique()
            unique_count = len(unique_values)
            st.write(f"**{cat_feat}**: {unique_count} å€‹å”¯ä¸€å€¼")
            st.write(f"  å€¼: {', '.join([str(v) for v in unique_values[:10]])}")
            if unique_count > 10:
                st.write(f"  ... é‚„æœ‰ {unique_count - 10} å€‹å€¼")
            st.write("---")
else:
    st.info("â„¹ï¸ æœªæª¢æ¸¬åˆ°é¡åˆ¥å‹ç‰¹å¾µï¼Œæ‰€æœ‰ç‰¹å¾µå°‡ä½œç‚ºæ•¸å€¼å‹è™•ç†ã€‚")

# å°‡é è™•ç†å¾Œçš„è³‡æ–™ä¿å­˜åˆ° session stateï¼Œä¾›å¾ŒçºŒä½¿ç”¨
st.session_state['preprocessed_df'] = df_preprocessed
st.session_state['thousand_separator_columns'] = thousand_separator_columns_detected
st.session_state['percentage_columns'] = percentage_columns_detected
st.session_state['categorical_features'] = categorical_features_detected

st.markdown("---")

# ========== æ­¥é©Ÿ 4: ç›®æ¨™è®Šæ•¸é¸æ“‡ ==========
st.markdown("### æ­¥é©Ÿ 4: é¸æ“‡ç›®æ¨™è®Šæ•¸")

# å¾é è™•ç†å¾Œçš„è³‡æ–™ä¸­å–å¾—æ‰€æœ‰æ•¸å€¼æ¬„ä½ï¼ˆåŒ…æ‹¬è½‰æ›å¾Œçš„ç™¾åˆ†æ¯”æ¬„ä½ï¼‰
# ä½¿ç”¨ session state ä¸­çš„é è™•ç†è³‡æ–™
if 'preprocessed_df' not in st.session_state:
    st.error("âŒ è«‹å…ˆå®Œæˆè³‡æ–™é è™•ç†è¨­å®šã€‚")
    st.stop()
df_preprocessed = st.session_state['preprocessed_df']
numeric_columns = df_preprocessed.select_dtypes(include=[np.number]).columns.tolist()

if not numeric_columns:
    st.error("âŒ é è™•ç†å¾Œçš„è³‡æ–™ä¸­æ²’æœ‰æ•¸å€¼æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ã€‚")
    st.stop()

# ç›®æ¨™è®Šæ•¸é¸æ“‡ï¼ˆå¤šé¸ï¼‰
target_variables = st.multiselect(
    "é¸æ“‡è¦é æ¸¬çš„æ¬„ä½ï¼ˆç›®æ¨™è®Šæ•¸ï¼‰",
    options=numeric_columns,
    help="å¯ä»¥é¸æ“‡ä¸€å€‹æˆ–å¤šå€‹æ¬„ä½ä½œç‚ºç›®æ¨™è®Šæ•¸ã€‚é¸æ“‡å¤šå€‹ç›®æ¨™è®Šæ•¸æ™‚ï¼Œæ¨¡å‹å°‡åŒæ™‚é æ¸¬æ‰€æœ‰ç›®æ¨™ã€‚æ³¨æ„ï¼šç™¾åˆ†æ¯”æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼ï¼Œå¯ä»¥é¸æ“‡ä½œç‚ºç›®æ¨™è®Šæ•¸ã€‚"
)

if not target_variables:
    st.warning("ğŸ‘† è«‹è‡³å°‘é¸æ“‡ä¸€å€‹ç›®æ¨™è®Šæ•¸")
    st.stop()

# é¡¯ç¤ºé¸æ“‡çš„ç›®æ¨™è®Šæ•¸è³‡è¨Š
if len(target_variables) > 1:
    st.success(f"âœ… å·²é¸æ“‡ {len(target_variables)} å€‹ç›®æ¨™è®Šæ•¸ï¼š{', '.join(target_variables)}")
    st.info("ğŸ’¡ å¤šç›®æ¨™è®Šæ•¸é æ¸¬ï¼šæ¨¡å‹å°‡åŒæ™‚å­¸ç¿’é æ¸¬æ‰€æœ‰é¸å®šçš„ç›®æ¨™è®Šæ•¸ã€‚æå¤±å‡½æ•¸å°‡åˆ†åˆ¥æ‡‰ç”¨åˆ°æ¯å€‹ç›®æ¨™è®Šæ•¸ã€‚")
else:
    st.info(f"âœ… å·²é¸æ“‡ç›®æ¨™è®Šæ•¸ï¼š{target_variables[0]}")

st.markdown("---")

# ========== æ­¥é©Ÿ 5: è¨“ç·´æ¼”ç®—æ³•é¸æ“‡ ==========
st.markdown("### æ­¥é©Ÿ 5: é¸æ“‡è¨“ç·´æ¼”ç®—æ³•")

algorithm = st.radio(
    "é¸æ“‡è¨“ç·´æ¼”ç®—æ³•",
    options=["ç·šæ€§å›æ­¸", "ç·šæ€§å›æ­¸æ¢¯åº¦ä¸‹é™"],
    help="ç·šæ€§å›æ­¸ï¼šå¿«é€Ÿç©©å®šï¼Œä½¿ç”¨æœ€å°äºŒä¹˜æ³•\nç·šæ€§å›æ­¸æ¢¯åº¦ä¸‹é™ï¼šå¯é¡¯ç¤ºè¨“ç·´éç¨‹ï¼Œæ”¯æ´ä¸åŒæå¤±å‡½å¼"
)

# å¦‚æœé¸æ“‡äº†å¤šå€‹ç›®æ¨™è®Šæ•¸ï¼Œé¡¯ç¤ºæå¤±å‡½æ•¸èªªæ˜
if len(target_variables) > 1:
    st.info(
        "ğŸ’¡ **å¤šç›®æ¨™è®Šæ•¸æå¤±å‡½æ•¸èªªæ˜**ï¼š\n\n"
        "ç•¶é æ¸¬å¤šå€‹ç›®æ¨™è®Šæ•¸æ™‚ï¼Œæå¤±å‡½æ•¸æœƒåˆ†åˆ¥æ‡‰ç”¨åˆ°æ¯å€‹ç›®æ¨™è®Šæ•¸ï¼š\n"
        "- **MSEï¼ˆå‡æ–¹èª¤å·®ï¼‰**ï¼šå°æ¯å€‹ç›®æ¨™è®Šæ•¸åˆ†åˆ¥è¨ˆç®— MSEï¼Œç„¶å¾Œå–å¹³å‡\n"
        "- **MAEï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼‰**ï¼šå°æ¯å€‹ç›®æ¨™è®Šæ•¸åˆ†åˆ¥è¨ˆç®— MAEï¼Œç„¶å¾Œå–å¹³å‡\n"
        "- **Huber**ï¼šå°æ¯å€‹ç›®æ¨™è®Šæ•¸åˆ†åˆ¥è¨ˆç®— Huber æå¤±ï¼Œç„¶å¾Œå–å¹³å‡\n\n"
        "æ¨¡å‹æœƒåŒæ™‚å„ªåŒ–æ‰€æœ‰ç›®æ¨™è®Šæ•¸çš„é æ¸¬æ•ˆæœã€‚"
    )

if algorithm == "ç·šæ€§å›æ­¸æ¢¯åº¦ä¸‹é™":
    use_scaling = st.checkbox(
        "ä½¿ç”¨è³‡æ–™æ¨™æº–åŒ–",
        value=True,
        help="æ¨™æº–åŒ–å¯ä»¥æ”¹å–„æ¢¯åº¦ä¸‹é™çš„æ”¶æ–‚é€Ÿåº¦å’Œæ•ˆæœã€‚å¼·çƒˆå»ºè­°é–‹å•Ÿï¼ˆé è¨­é–‹å•Ÿï¼‰ã€‚"
    )
else:
    use_scaling = False

st.markdown("---")

# ========== æ­¥é©Ÿ 6: åƒæ•¸è¨­å®š ==========
st.markdown("### æ­¥é©Ÿ 6: è¨­å®šè¨“ç·´åƒæ•¸")

# åˆå§‹åŒ–æ­£å‰‡åŒ–è®Šæ•¸ï¼ˆç¢ºä¿åœ¨æ‰€æœ‰æƒ…æ³ä¸‹éƒ½æœ‰å®šç¾©ï¼‰
regularization_type = "ç„¡æ­£å‰‡åŒ–"
alpha = 1.0

# ç·šæ€§å›æ­¸çš„æ­£å‰‡åŒ–é¸é …
if algorithm == "ç·šæ€§å›æ­¸":
    regularization_type = st.radio(
        "æ­£å‰‡åŒ–é¡å‹",
        options=["ç„¡æ­£å‰‡åŒ–", "L1 (Lasso)", "L2 (Ridge)"],
        help="æ­£å‰‡åŒ–å¯ä»¥å¹«åŠ©é˜²æ­¢éæ“¬åˆï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›"
    )
    
    # æ­£å‰‡åŒ–å¼·åº¦è¨­å®šï¼ˆåƒ…åœ¨é¸æ“‡ L1 æˆ– L2 æ™‚é¡¯ç¤ºï¼‰
    if regularization_type != "ç„¡æ­£å‰‡åŒ–":
        alpha = st.slider(
            "æ­£å‰‡åŒ–å¼·åº¦ (alpha)",
            min_value=0.001,
            max_value=100.0,
            value=1.0,
            step=0.1,
            format="%.3f",
            help="alpha å€¼è¶Šå¤§ï¼Œæ­£å‰‡åŒ–æ•ˆæœè¶Šå¼·ã€‚å»ºè­°å¾ 1.0 é–‹å§‹å˜—è©¦ã€‚"
        )
    
    # å¯å±•é–‹/æ”¶èµ·çš„æ­£å‰‡åŒ–èªªæ˜
    with st.expander("ğŸ“– æ­£å‰‡åŒ–èªªæ˜ï¼ˆé»æ“Šå±•é–‹ï¼‰"):
        st.markdown("""
        **ä»€éº¼æ˜¯æ­£å‰‡åŒ–ï¼Ÿ**
        
        æ­£å‰‡åŒ–æ˜¯ä¸€ç¨®é˜²æ­¢æ¨¡å‹éæ“¬åˆçš„æŠ€è¡“ã€‚ç•¶æ¨¡å‹éæ–¼è¤‡é›œæ™‚ï¼Œå¯èƒ½æœƒéåº¦å­¸ç¿’è¨“ç·´è³‡æ–™çš„ç´°ç¯€ï¼Œå°è‡´åœ¨æ–°è³‡æ–™ä¸Šè¡¨ç¾ä¸ä½³ã€‚æ­£å‰‡åŒ–é€šéæ·»åŠ æ‡²ç½°é …ä¾†æ§åˆ¶æ¨¡å‹çš„è¤‡é›œåº¦ã€‚
        
        **L1 æ­£å‰‡åŒ– (Lasso)**
        - **ä½œç”¨**ï¼šæœƒå°‡æŸäº›ç‰¹å¾µçš„ä¿‚æ•¸ç¸®æ¸›ç‚º 0ï¼Œå¯¦ç¾ç‰¹å¾µé¸æ“‡
        - **ç‰¹é»**ï¼šé©åˆç•¶æ‚¨èªç‚ºåªæœ‰éƒ¨åˆ†ç‰¹å¾µé‡è¦æ™‚ä½¿ç”¨
        - **æ•ˆæœ**ï¼šå¯ä»¥è‡ªå‹•æ’é™¤ä¸é‡è¦çš„ç‰¹å¾µï¼Œç°¡åŒ–æ¨¡å‹
        
        **L2 æ­£å‰‡åŒ– (Ridge)**
        - **ä½œç”¨**ï¼šæœƒç¸®å°æ‰€æœ‰ç‰¹å¾µçš„ä¿‚æ•¸ï¼Œä½†ä¸æœƒå®Œå…¨æ¶ˆé™¤
        - **ç‰¹é»**ï¼šé©åˆç•¶æ‚¨èªç‚ºæ‰€æœ‰ç‰¹å¾µéƒ½æœ‰ä¸€å®šé‡è¦æ€§æ™‚ä½¿ç”¨
        - **æ•ˆæœ**ï¼šè®“æ¨¡å‹åƒæ•¸æ›´å¹³æ»‘ï¼Œæ¸›å°‘æ¥µç«¯å€¼
        
        **å¦‚ä½•é¸æ“‡ï¼Ÿ**
        - **é¸æ“‡ L1**ï¼šç•¶æ‚¨æƒ³è¦è‡ªå‹•é¸æ“‡é‡è¦ç‰¹å¾µï¼Œæˆ–ç‰¹å¾µæ•¸é‡å¾ˆå¤šæ™‚
        - **é¸æ“‡ L2**ï¼šç•¶æ‚¨æƒ³è¦ä¿ç•™æ‰€æœ‰ç‰¹å¾µï¼Œä½†å¸Œæœ›æ¨¡å‹æ›´ç©©å®šæ™‚
        - **ç„¡æ­£å‰‡åŒ–**ï¼šç•¶è³‡æ–™é‡è¶³å¤ å¤§ï¼Œæˆ–ä¸éœ€è¦æ§åˆ¶éæ“¬åˆæ™‚
        """)
    
    st.markdown("---")

if algorithm == "ç·šæ€§å›æ­¸æ¢¯åº¦ä¸‹é™":
    col1, col2 = st.columns(2)
    
    with col1:
        loss_function = st.selectbox(
            "æå¤±å‡½å¼",
            options=["MSE", "MAE", "Huber"],
            help="MSEï¼šå‡æ–¹èª¤å·®ï¼ˆå¸¸ç”¨ï¼‰\nMAEï¼šå¹³å‡çµ•å°èª¤å·®\nHuberï¼šå°ç•°å¸¸å€¼è¼ƒä¸æ•æ„Ÿ"
        )
        
        learning_rate = st.number_input(
            "å­¸ç¿’ç‡",
            min_value=0.0001,
            max_value=1.0,
            value=0.01,
            step=0.001,
            help="å­¸ç¿’ç‡æ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•·"
        )
    
    with col2:
        max_iter = st.number_input(
            "æœ€å¤§è¿­ä»£æ¬¡æ•¸",
            min_value=10,
            max_value=10000,
            value=1000,
            step=10,
            help="è¨“ç·´çš„æœ€å¤§è¿­ä»£æ¬¡æ•¸"
        )
        
        tol = st.number_input(
            "æ”¶æ–‚å®¹å¿åº¦",
            min_value=1e-10,
            max_value=1e-3,
            value=1e-6,
            format="%e",
            help="ç•¶æå¤±è®ŠåŒ–å°æ–¼æ­¤å€¼æ™‚åœæ­¢è¨“ç·´"
        )
else:
    loss_function = None
    learning_rate = None
    max_iter = None
    tol = None

st.markdown("---")

# ========== æ­¥é©Ÿ 7: è³‡æ–™åˆ†å‰²è¨­å®š ==========
st.markdown("### æ­¥é©Ÿ 7: è³‡æ–™åˆ†å‰²è¨­å®š")

split_data = st.checkbox(
    "åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†",
    value=False,
    help="é¸æ“‡æ˜¯å¦å°‡è³‡æ–™åˆ†å‰²ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ã€‚åˆ†å‰²å¾Œï¼Œæ¨¡å‹å°‡åœ¨è¨“ç·´é›†ä¸Šè¨“ç·´ï¼Œä¸¦åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ã€‚"
)

test_size = 0.2  # é è¨­æ¸¬è©¦é›†æ¯”ä¾‹
if split_data:
    test_size = st.slider(
        "æ¸¬è©¦é›†æ¯”ä¾‹",
        min_value=0.1,
        max_value=0.5,
        value=0.2,
        step=0.05,
        help="æ¸¬è©¦é›†ä½”ç¸½è³‡æ–™çš„æ¯”ä¾‹ï¼ˆä¾‹å¦‚ 0.2 è¡¨ç¤º 20% çš„è³‡æ–™ä½œç‚ºæ¸¬è©¦é›†ï¼‰"
    )
    st.info(f"ğŸ“Š è³‡æ–™å°‡åˆ†å‰²ç‚ºï¼šè¨“ç·´é›† {int((1-test_size)*100)}%ï¼Œæ¸¬è©¦é›† {int(test_size*100)}%")
    
    # è©•ä¼°æ–¹å¼é¸æ“‡ï¼ˆåƒ…åœ¨åˆ†å‰²è³‡æ–™æ™‚é¡¯ç¤ºï¼‰
    evaluation_method = st.radio(
        "è©•ä¼°æ–¹å¼",
        options=["single", "repeated"],
        format_func=lambda x: "å–®æ¬¡è©•ä¼°" if x == "single" else "é‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°",
        help="å–®æ¬¡è©•ä¼°ï¼šå¿«é€Ÿè©•ä¼°ï¼Œé©åˆå¿«é€Ÿæ¸¬è©¦\né‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°ï¼šå¤šæ¬¡è©•ä¼°å–å¹³å‡ï¼Œçµæœæ›´ç©©å®šå¯é "
    )
    
    n_repeats = 5  # é è¨­é‡è¤‡æ¬¡æ•¸
    if evaluation_method == "repeated":
        n_repeats = st.number_input(
            "é‡è¤‡æ¬¡æ•¸",
            min_value=3,
            max_value=20,
            value=5,
            step=1,
            help="é‡è¤‡è©•ä¼°çš„æ¬¡æ•¸ã€‚æ¬¡æ•¸è¶Šå¤šï¼Œçµæœè¶Šç©©å®šï¼Œä½†éœ€è¦æ›´é•·çš„æ™‚é–“"
        )
        st.info(
            "ğŸ’¡ **é‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°èªªæ˜**ï¼š\n\n"
            "- **ä½œç”¨**ï¼šé€éå¤šæ¬¡éš¨æ©Ÿåˆ†å‰²å’Œè©•ä¼°ä¾†ç²å¾—æ›´ç©©å®šã€å¯é çš„è©•ä¼°çµæœ\n"
            "- **å„ªé»**ï¼šæ¸›å°‘å–®æ¬¡éš¨æ©Ÿåˆ†å‰²çš„åå·®ï¼Œæä¾›è©•ä¼°æŒ‡æ¨™çš„è®Šç•°æ€§è³‡è¨Š\n"
            "- **çµæœ**ï¼šé¡¯ç¤ºå¹³å‡å€¼ Â± æ¨™æº–å·®ï¼Œè®“æ‚¨äº†è§£è©•ä¼°çµæœçš„ç©©å®šæ€§\n"
            "- **æ³¨æ„**ï¼šé‡è¤‡è©•ä¼°æœƒå¢åŠ è¨“ç·´æ™‚é–“ï¼ˆç´„ N å€ï¼‰ï¼Œè«‹æ ¹æ“šè³‡æ–™å¤§å°é¸æ“‡åˆé©çš„é‡è¤‡æ¬¡æ•¸"
        )
else:
    # å¦‚æœæ²’æœ‰åˆ†å‰²è³‡æ–™ï¼Œä½¿ç”¨å–®æ¬¡è©•ä¼°
    evaluation_method = "single"
    n_repeats = 1

st.markdown("---")

# ========== æ­¥é©Ÿ 8: è³‡æ–™æ“´å¢è¨­å®š ==========
st.markdown("### æ­¥é©Ÿ 8: è³‡æ–™æ“´å¢è¨­å®šï¼ˆå¯é¸ï¼‰")

enable_augmentation = st.checkbox(
    "å•Ÿç”¨è³‡æ–™æ“´å¢",
    value=False,
    help="è³‡æ–™æ“´å¢å¯ä»¥é€éå°æ•¸å€¼æ¬„ä½æ·»åŠ å™ªè²ä¾†å¢åŠ è¨“ç·´æ¨£æœ¬æ•¸é‡ï¼Œæœ‰åŠ©æ–¼æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›"
)

augmentation_params = {}
if enable_augmentation:
    st.info(
        "ğŸ’¡ **è³‡æ–™æ“´å¢èªªæ˜**ï¼š\n\n"
        "- **ä½œç”¨**ï¼šé€éå°æ•¸å€¼æ¬„ä½æ·»åŠ é©åº¦çš„å™ªè²ä¾†å¢åŠ è¨“ç·´æ¨£æœ¬æ•¸é‡\n"
        "- **é©ç”¨æ¬„ä½**ï¼šåƒ…å°æ•¸å€¼å‹æ¬„ä½é€²è¡Œæ“´å¢ï¼Œé¡åˆ¥å‹æ¬„ä½ä¿æŒä¸è®Š\n"
        "- **æ“´å¢ç¯„åœ**ï¼šåƒ…å°è¨“ç·´é›†é€²è¡Œæ“´å¢ï¼Œæ¸¬è©¦é›†ä¿æŒåŸæ¨£ä»¥ç¢ºä¿è©•ä¼°æº–ç¢ºæ€§\n"
        "- **å»ºè­°**ï¼šç•¶è¨“ç·´è³‡æ–™é‡è¼ƒå°‘æ™‚ï¼Œå¯ä»¥ä½¿ç”¨è³‡æ–™æ“´å¢ä¾†æå‡æ¨¡å‹æ•ˆæœ"
    )
    
    col1, col2 = st.columns(2)
    
    with col1:
        noise_type = st.radio(
            "å™ªè²é¡å‹",
            options=["gaussian", "uniform"],
            format_func=lambda x: "é«˜æ–¯å™ªè²ï¼ˆGaussianï¼‰" if x == "gaussian" else "å‡å‹»å™ªè²ï¼ˆUniformï¼‰",
            help="é«˜æ–¯å™ªè²ï¼šç¬¦åˆå¸¸æ…‹åˆ†å¸ƒï¼Œé©åˆå¤§å¤šæ•¸æƒ…æ³\nå‡å‹»å™ªè²ï¼šåœ¨å›ºå®šç¯„åœå…§å‡å‹»åˆ†å¸ƒ"
        )
        
        noise_strength = st.slider(
            "å™ªè²å¼·åº¦",
            min_value=0.01,
            max_value=0.5,
            value=0.1,
            step=0.01,
            help="å™ªè²å¼·åº¦ç›¸å°æ–¼æ¬„ä½æ¨™æº–å·®çš„æ¯”ä¾‹ã€‚å€¼è¶Šå¤§ï¼Œæ·»åŠ çš„å™ªè²è¶Šå¤š"
        )
    
    with col2:
        multiplier = st.number_input(
            "æ“´å¢å€æ•¸",
            min_value=1,
            max_value=5,
            value=2,
            step=1,
            help="å°‡è¨“ç·´è³‡æ–™æ“´å¢ç‚ºåŸä¾†çš„å¹¾å€ã€‚ä¾‹å¦‚ï¼š2 å€è¡¨ç¤ºè³‡æ–™é‡ç¿»å€"
        )
    
    augmentation_params = {
        'noise_type': noise_type,
        'noise_strength': noise_strength,
        'multiplier': multiplier
    }

st.markdown("---")

# ========== æ­¥é©Ÿ 9: è¨“ç·´åŸ·è¡Œ ==========
st.markdown("### æ­¥é©Ÿ 9: é–‹å§‹è¨“ç·´")

if st.button("ğŸš€ é–‹å§‹è¨“ç·´", type="primary", use_container_width=True):
    with st.spinner("æ­£åœ¨è¨“ç·´æ¨¡å‹..."):
        try:
            # æº–å‚™è³‡æ–™
            # ä½¿ç”¨é è™•ç†å¾Œçš„è³‡æ–™ï¼ˆå¾ session state å–å¾—ï¼‰
            if 'preprocessed_df' not in st.session_state:
                st.error("âŒ è«‹å…ˆå®Œæˆè³‡æ–™é è™•ç†è¨­å®šã€‚")
                st.stop()
            df_processed = st.session_state['preprocessed_df']
            
            # ç‰¹å¾µè®Šæ•¸ = æ‰€æœ‰å¯ç”¨æ¬„ä½ - ç›®æ¨™è®Šæ•¸
            feature_columns = [
                col for col in df_processed.columns 
                if col not in target_variables
            ]
            
            if not feature_columns:
                st.error("âŒ æ²’æœ‰å¯ç”¨çš„ç‰¹å¾µè®Šæ•¸ã€‚è«‹ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ¬„ä½ä½œç‚ºç‰¹å¾µè®Šæ•¸ï¼ˆæ’é™¤ç›®æ¨™è®Šæ•¸ï¼‰ã€‚")
                st.stop()
            
            X = df_processed[feature_columns].copy()
            y = df_processed[target_variables].copy()
            
            # é¡¯ç¤ºä½¿ç”¨çš„ç‰¹å¾µè®Šæ•¸è³‡è¨Š
            st.info(f"ğŸ“Š ä½¿ç”¨ {len(feature_columns)} å€‹ç‰¹å¾µè®Šæ•¸ï¼š{', '.join(feature_columns)}")
            
            # è™•ç†ç¼ºå¤±å€¼ï¼ˆç°¡å–®ç­–ç•¥ï¼šç§»é™¤ï¼‰
            X = X.dropna()
            y = y.loc[X.index]
            
            if len(X) == 0:
                st.error("âŒ è³‡æ–™ä¸­ç¼ºå¤±å€¼éå¤šï¼Œç„¡æ³•é€²è¡Œè¨“ç·´ã€‚")
                st.stop()
            
            # è³‡æ–™åˆ†å‰²ï¼ˆå¦‚æœéœ€è¦ï¼‰
            X_train = X
            y_train = y
            X_test = None
            y_test = None
            
            if split_data:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, 
                    test_size=test_size, 
                    random_state=42,
                    shuffle=True
                )
                st.success(f"âœ… è³‡æ–™å·²åˆ†å‰²ï¼šè¨“ç·´é›† {len(X_train)} ç­†ï¼Œæ¸¬è©¦é›† {len(X_test)} ç­†")
            
            # è³‡æ–™æ“´å¢ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if enable_augmentation:
                from utils.data_augmenter import augment_dataframe, get_augmentation_stats
                
                st.markdown("#### ğŸ“ˆ åŸ·è¡Œè³‡æ–™æ“´å¢")
                with st.spinner("æ­£åœ¨æ“´å¢è¨“ç·´è³‡æ–™..."):
                    try:
                        # ä¿å­˜æ“´å¢å‰çš„è³‡æ–™ç”¨æ–¼çµ±è¨ˆ
                        X_train_before = X_train.copy()
                        y_train_before = y_train.copy()
                        
                        # å–å¾—é¡åˆ¥å‹ç‰¹å¾µåˆ—è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        categorical_features_for_aug = st.session_state.get('categorical_features', None)
                        # ç¢ºä¿é¡åˆ¥å‹ç‰¹å¾µåªåŒ…å«åœ¨ X_train ä¸­çš„ç‰¹å¾µ
                        if categorical_features_for_aug:
                            categorical_features_for_aug = [f for f in categorical_features_for_aug if f in X_train.columns]
                        
                        # æ“´å¢ç‰¹å¾µè³‡æ–™ï¼ˆæ’é™¤é¡åˆ¥å‹ç‰¹å¾µï¼‰
                        X_train = augment_dataframe(
                            X_train,
                            noise_type=augmentation_params['noise_type'],
                            noise_strength=augmentation_params['noise_strength'],
                            multiplier=augmentation_params['multiplier'],
                            random_state=42,
                            categorical_features=categorical_features_for_aug
                        )
                        
                        # æ“´å¢ç›®æ¨™è®Šæ•¸ï¼ˆç¢ºä¿èˆ‡ X_train çš„è¡Œæ•¸ä¸€è‡´ï¼‰
                        # ç”±æ–¼ X_train å·²ç¶“æ“´å¢ï¼Œy_train éœ€è¦å°æ‡‰æ“´å¢
                        if y_train.select_dtypes(include=[np.number]).shape[1] > 0:
                            # å¦‚æœç›®æ¨™è®Šæ•¸æ˜¯æ•¸å€¼å‹ï¼Œä½¿ç”¨å™ªè²æ“´å¢
                            y_train = augment_dataframe(
                                y_train_before,
                                noise_type=augmentation_params['noise_type'],
                                noise_strength=augmentation_params['noise_strength'],
                                multiplier=augmentation_params['multiplier'],
                                random_state=42
                            )
                        else:
                            # å¦‚æœç›®æ¨™è®Šæ•¸ä¸æ˜¯æ•¸å€¼å‹ï¼Œå‰‡é‡è¤‡å°æ‡‰æ¬¡æ•¸
                            y_train = pd.concat([y_train_before] * augmentation_params['multiplier'], ignore_index=True)
                        
                        # ç¢ºä¿ X_train å’Œ y_train çš„è¡Œæ•¸ä¸€è‡´
                        if len(X_train) != len(y_train):
                            # å¦‚æœè¡Œæ•¸ä¸ä¸€è‡´ï¼Œèª¿æ•´ y_train ä»¥åŒ¹é… X_train
                            min_len = min(len(X_train), len(y_train))
                            X_train = X_train.iloc[:min_len]
                            y_train = y_train.iloc[:min_len]
                        
                        # é¡¯ç¤ºæ“´å¢çµ±è¨ˆè³‡è¨Š
                        st.success(f"âœ… è³‡æ–™æ“´å¢å®Œæˆï¼šè¨“ç·´é›†å¾ {len(X_train_before)} ç­†æ“´å¢åˆ° {len(X_train)} ç­†")
                        
                        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
                        if len(X_train_before.select_dtypes(include=[np.number]).columns) > 0:
                            stats_df = get_augmentation_stats(X_train_before, X_train)
                            st.markdown("**æ“´å¢å‰å¾Œçµ±è¨ˆè³‡è¨Šï¼š**")
                            st.dataframe(stats_df, use_container_width=True)
                    
                    except Exception as e:
                        st.warning(f"âš ï¸ è³‡æ–™æ“´å¢å¤±æ•—ï¼š{str(e)}\n\nå°‡ä½¿ç”¨åŸå§‹è³‡æ–™é€²è¡Œè¨“ç·´ã€‚")
                        # å¦‚æœæ“´å¢å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™
                        X_train = X_train_before if 'X_train_before' in locals() else X_train
                        y_train = y_train_before if 'y_train_before' in locals() else y_train
            
            # æ ¹æ“šé¸æ“‡çš„æ¼”ç®—æ³•å»ºç«‹æ¨¡å‹
            # å–å¾—æª¢æ¸¬åˆ°çš„é¡åˆ¥å‹ç‰¹å¾µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            categorical_features = st.session_state.get('categorical_features', None)
            # ç¢ºä¿é¡åˆ¥å‹ç‰¹å¾µåªåŒ…å«åœ¨ X_train ä¸­çš„ç‰¹å¾µ
            if categorical_features:
                categorical_features = [f for f in categorical_features if f in X_train.columns]
            
            if algorithm == "ç·šæ€§å›æ­¸":
                # æ ¹æ“šé¸æ“‡çš„æ­£å‰‡åŒ–é¡å‹è¨­å®šåƒæ•¸
                regularization = None
                if regularization_type == "L1 (Lasso)":
                    regularization = 'l1'
                elif regularization_type == "L2 (Ridge)":
                    regularization = 'l2'
                
                model = LinearRegressionModel(
                    regularization=regularization,
                    alpha=alpha if regularization else 1.0
                )
                model.fit(X_train, y_train, categorical_features=categorical_features)
                
                # é¡¯ç¤ºé è™•ç†è³‡è¨Š
                preprocessing_info = []
                if model.preprocessing_metadata.get('thousand_separator_columns'):
                    ts_features = model.preprocessing_metadata['thousand_separator_columns']
                    preprocessing_info.append(f"âœ… {len(ts_features)} å€‹åƒåˆ†ä½åˆ†éš”ç¬¦æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼ï¼š{', '.join(ts_features)}")
                if model.preprocessing_metadata.get('percentage_columns'):
                    pct_features = model.preprocessing_metadata['percentage_columns']
                    preprocessing_info.append(f"âœ… {len(pct_features)} å€‹ç™¾åˆ†æ¯”æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼ï¼š{', '.join(pct_features)}")
                if model.preprocessing_metadata.get('categorical_features'):
                    cat_features = model.preprocessing_metadata['categorical_features']
                    preprocessing_info.append(f"âœ… {len(cat_features)} å€‹é¡åˆ¥å‹ç‰¹å¾µå·²é€²è¡Œç¨ç†±ç·¨ç¢¼ï¼š{', '.join(cat_features)}")
                if preprocessing_info:
                    st.info("\n".join(preprocessing_info))
            else:
                model = GradientDescentModel(
                    loss=loss_function,
                    learning_rate=learning_rate,
                    max_iter=max_iter,
                    tol=tol,
                    use_scaling=use_scaling
                )
                model.fit(X_train, y_train, record_history=True, categorical_features=categorical_features)
                
                # é¡¯ç¤ºé è™•ç†è³‡è¨Š
                preprocessing_info = []
                if model.preprocessing_metadata.get('thousand_separator_columns'):
                    ts_features = model.preprocessing_metadata['thousand_separator_columns']
                    preprocessing_info.append(f"âœ… {len(ts_features)} å€‹åƒåˆ†ä½åˆ†éš”ç¬¦æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼ï¼š{', '.join(ts_features)}")
                if model.preprocessing_metadata.get('percentage_columns'):
                    pct_features = model.preprocessing_metadata['percentage_columns']
                    preprocessing_info.append(f"âœ… {len(pct_features)} å€‹ç™¾åˆ†æ¯”æ¬„ä½å·²è½‰æ›ç‚ºæ•¸å€¼ï¼š{', '.join(pct_features)}")
                if model.preprocessing_metadata.get('categorical_features'):
                    cat_features = model.preprocessing_metadata['categorical_features']
                    preprocessing_info.append(f"âœ… {len(cat_features)} å€‹é¡åˆ¥å‹ç‰¹å¾µå·²é€²è¡Œç¨ç†±ç·¨ç¢¼ï¼š{', '.join(cat_features)}")
                if use_scaling:
                    preprocessing_info.append("âœ… æ•¸å€¼å‹ç‰¹å¾µå·²é€²è¡Œæ¨™æº–åŒ–")
                if preprocessing_info:
                    st.info("\n".join(preprocessing_info))
            
            # å„²å­˜æ¨¡å‹åˆ° session state
            st.session_state['trained_model'] = model
            st.session_state['training_X'] = X_train
            st.session_state['training_y'] = y_train
            st.session_state['test_X'] = X_test
            st.session_state['test_y'] = y_test
            st.session_state['split_data'] = split_data
            
            st.success("âœ… æ¨¡å‹è¨“ç·´å®Œæˆï¼")
            
        except Exception as e:
            st.error(f"âŒ è¨“ç·´å¤±æ•—ï¼š{str(e)}")
            st.stop()

# ========== æ­¥é©Ÿ 9: è¨“ç·´çµæœé¡¯ç¤º ==========
if 'trained_model' in st.session_state and st.session_state['trained_model'] is not None:
    st.markdown("---")
    st.markdown("### æ­¥é©Ÿ 9: è¨“ç·´çµæœ")
    
    model = st.session_state['trained_model']
    X_train = st.session_state['training_X']
    y_train = st.session_state['training_y']
    split_data = st.session_state.get('split_data', False)
    X_test = st.session_state.get('test_X', None)
    y_test = st.session_state.get('test_y', None)
    
    # å–å¾—æ¨¡å‹è³‡è¨Š
    model_info = model.get_info()
    
    # é¡¯ç¤ºæ¨¡å‹åŸºæœ¬è³‡è¨Š
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("æ¨¡å‹é¡å‹", model_info['model_name'])
    with col2:
        st.metric("ç›®æ¨™è®Šæ•¸æ•¸é‡", len(model_info['target_names']))
    with col3:
        st.metric("ç‰¹å¾µæ•¸é‡", model_info['n_features'])
    with col4:
        if split_data:
            st.metric("è³‡æ–™åˆ†å‰²", "æ˜¯")
        else:
            st.metric("è³‡æ–™åˆ†å‰²", "å¦")
    with col5:
        # é¡¯ç¤ºæ­£å‰‡åŒ–è³‡è¨Šï¼ˆåƒ…ç·šæ€§å›æ­¸ï¼‰
        if isinstance(model, LinearRegressionModel) and model_info.get('regularization'):
            reg_type = model_info['regularization']
            if reg_type == 'l1':
                reg_display = "L1 (Lasso)"
            elif reg_type == 'l2':
                reg_display = "L2 (Ridge)"
            else:
                reg_display = "ç„¡"
            st.metric("æ­£å‰‡åŒ–", reg_display)
        else:
            st.metric("æ­£å‰‡åŒ–", "ç„¡")
    
    # é¡¯ç¤ºæ­£å‰‡åŒ–å¼·åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
    if isinstance(model, LinearRegressionModel) and model_info.get('regularization') and model_info.get('alpha'):
        st.info(f"ğŸ“Œ **æ­£å‰‡åŒ–è¨­å®š**ï¼š{model_info['regularization'].upper()} æ­£å‰‡åŒ–ï¼Œå¼·åº¦ alpha = {model_info['alpha']:.3f}")
    
    # æå¤±æ›²ç·šï¼ˆåƒ…æ¢¯åº¦ä¸‹é™ï¼‰
    if isinstance(model, GradientDescentModel) and model.training_history:
        st.markdown("#### ğŸ“Š è¨“ç·´æå¤±æ›²ç·š")
        loss_fig = plot_training_loss(model.training_history, model.loss)
        st.plotly_chart(loss_fig, use_container_width=True)
    
    # æ¨¡å‹åƒæ•¸ï¼ˆä¿‚æ•¸ï¼‰
    st.markdown("#### ğŸ“ˆ æ¨¡å‹åƒæ•¸ï¼ˆä¿‚æ•¸ï¼‰")
    
    # å˜—è©¦é¡¯ç¤ºå¸¶ p å€¼çš„ä¿‚æ•¸è¡¨æ ¼ï¼ˆåƒ…ç·šæ€§å›æ­¸ä¸”å–®ä¸€ç›®æ¨™è®Šæ•¸ï¼‰
    show_pvalues = (
        isinstance(model, LinearRegressionModel) and 
        not model_info.get('is_multi_output', False) and
        X_train is not None and y_train is not None
    )
    
    if show_pvalues:
        try:
            # é‡æ–°æ‡‰ç”¨é è™•ç†ä»¥ç²å–é è™•ç†å¾Œçš„è³‡æ–™ï¼ˆèˆ‡æ¨¡å‹è¨“ç·´æ™‚ä¸€è‡´ï¼‰
            # é€™æ¨£æ‰èƒ½æ­£ç¢ºè¨ˆç®— p å€¼å’Œé€²è¡Œè¨ºæ–·
            X_processed = None
            if hasattr(model, 'preprocessing_metadata') and hasattr(model, 'encoder'):
                from utils.data_preprocessor import preprocess_features
                # é‡æ–°æ‡‰ç”¨é è™•ç†ï¼ˆä½¿ç”¨ fit=Falseï¼Œå› ç‚ºæ¨¡å‹å·²ç¶“è¨“ç·´éï¼‰
                X_processed, _, _, _ = preprocess_features(
                    X_train,
                    categorical_features=model.preprocessing_metadata.get('categorical_features', []),
                    use_scaling=False,  # ç·šæ€§å›æ­¸ä¸ä½¿ç”¨æ¨™æº–åŒ–
                    fit=False,
                    scaler=None,
                    encoder=model.encoder
                )
                X_train_values = X_processed.values
            else:
                # å¦‚æœæ²’æœ‰é è™•ç†å…ƒè³‡æ–™ï¼Œä½¿ç”¨åŸå§‹è³‡æ–™
                X_train_values = X_train.values if isinstance(X_train, pd.DataFrame) else X_train
            
            y_train_values = y_train.values if isinstance(y_train, pd.DataFrame) else y_train
            
            # ç¢ºä¿è³‡æ–™æ˜¯æ•¸å€¼å‹ä¸”æ²’æœ‰ç„¡é™å€¼æˆ– NaN
            X_train_values = np.array(X_train_values, dtype=np.float64)
            y_train_values = np.array(y_train_values, dtype=np.float64).flatten()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç„¡æ•ˆå€¼
            if np.any(np.isnan(X_train_values)) or np.any(np.isinf(X_train_values)):
                raise ValueError("X è³‡æ–™åŒ…å« NaN æˆ– Inf å€¼")
            if np.any(np.isnan(y_train_values)) or np.any(np.isinf(y_train_values)):
                raise ValueError("y è³‡æ–™åŒ…å« NaN æˆ– Inf å€¼")
            
            params_df = display_model_parameters_with_pvalues(
                model_info, 
                X_train_values, 
                y_train_values
            )
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ p å€¼è¨ˆç®—æˆåŠŸ
            p_value_col = 'p å€¼' if 'p å€¼' in params_df.columns else None
            if p_value_col:
                valid_p_count = params_df[p_value_col].apply(lambda x: x != 'N/A' and not pd.isna(x) if isinstance(x, str) else not pd.isna(x)).sum()
                total_count = len(params_df)
                
                if valid_p_count == 0:
                    # åŸ·è¡Œè¨ºæ–·ä»¥æä¾›æ›´å…·é«”çš„ä¿¡æ¯
                    from utils.visualizer import diagnose_pvalue_issues
                    
                    # ç²å–ç‰¹å¾µåç¨± - å„ªå…ˆä½¿ç”¨é è™•ç†å¾Œè³‡æ–™çš„åˆ—åï¼ˆæœ€æº–ç¢ºï¼‰
                    if X_processed is not None and isinstance(X_processed, pd.DataFrame):
                        feature_names = list(X_processed.columns)
                    else:
                        # å¦‚æœæ²’æœ‰é è™•ç†å¾Œçš„ DataFrameï¼Œä½¿ç”¨ model_info ä¸­çš„ç‰¹å¾µåç¨±
                        feature_names = model_info.get('feature_names', [])
                        
                        # å¦‚æœç‰¹å¾µåç¨±ä¸åŒ¹é…ï¼Œå˜—è©¦å¾é è™•ç†å…ƒè³‡æ–™æ§‹å»º
                        if not feature_names or len(feature_names) != X_train_values.shape[1]:
                            if hasattr(model, 'preprocessing_metadata'):
                                metadata = model.preprocessing_metadata
                                numeric_names = metadata.get('numeric_features', [])
                                encoded_names = metadata.get('encoded_feature_names', [])
                                
                                # æ§‹å»ºç‰¹å¾µåç¨±åˆ—è¡¨ï¼šæ•¸å€¼ç‰¹å¾µ + ç·¨ç¢¼å¾Œçš„é¡åˆ¥ç‰¹å¾µ
                                feature_names = numeric_names + encoded_names
                    
                    # å¦‚æœä»ç„¶ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜èªåç¨±
                    if not feature_names or len(feature_names) != X_train_values.shape[1]:
                        feature_names = None  # è®“è¨ºæ–·å‡½æ•¸ä½¿ç”¨é»˜èªå‘½å
                    
                    diagnostics = diagnose_pvalue_issues(X_train_values, y_train_values, feature_names)
                    
                    warning_msg = "âš ï¸ **æ‰€æœ‰ p å€¼éƒ½ç„¡æ³•è¨ˆç®—**\n\n"
                    warning_msg += "**è¨ºæ–·çµæœï¼š**\n"
                    warning_msg += f"- æ¨£æœ¬æ•¸ï¼š{diagnostics['n_samples']}\n"
                    warning_msg += f"- ç‰¹å¾µæ•¸ï¼š{diagnostics['n_features']}\n"
                    warning_msg += f"- æ¨£æœ¬/ç‰¹å¾µæ¯”ï¼š{diagnostics['sample_feature_ratio']:.2f}\n"
                    warning_msg += f"- è‡ªç”±åº¦ï¼š{diagnostics['degrees_of_freedom']}\n"
                    
                    if diagnostics['condition_number'] is not None:
                        warning_msg += f"- æ¢ä»¶æ•¸ï¼š{diagnostics['condition_number']:.2e}\n"
                    
                    if diagnostics['issues']:
                        warning_msg += "\n**ç™¼ç¾çš„å•é¡Œï¼š**\n"
                        for issue in diagnostics['issues']:
                            warning_msg += f"- {issue}\n"
                    
                    # é¡¯ç¤ºå…·é«”çš„å•é¡Œç‰¹å¾µ
                    if diagnostics['constant_feature_names']:
                        warning_msg += f"\n**å¸¸æ•¸ç‰¹å¾µåˆ—è¡¨ï¼š**\n"
                        for name in diagnostics['constant_feature_names']:
                            warning_msg += f"- `{name}`\n"
                    
                    if diagnostics['highly_correlated_pairs']:
                        warning_msg += f"\n**é«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°ï¼ˆ|r| > 0.95ï¼‰ï¼š**\n"
                        for pair in diagnostics['highly_correlated_pairs'][:10]:  # æœ€å¤šé¡¯ç¤º10å°
                            corr_val = pair['correlation']
                            warning_msg += f"- `{pair['feature1']}` èˆ‡ `{pair['feature2']}` (r = {corr_val:.4f})\n"
                        if len(diagnostics['highly_correlated_pairs']) > 10:
                            warning_msg += f"- ... é‚„æœ‰ {len(diagnostics['highly_correlated_pairs']) - 10} å°é«˜åº¦ç›¸é—œçš„ç‰¹å¾µ\n"
                    
                    warning_msg += "\n**å»ºè­°è§£æ±ºæ–¹æ¡ˆï¼š**\n"
                    if diagnostics['n_samples'] <= diagnostics['n_features']:
                        warning_msg += "- **å¢åŠ æ¨£æœ¬æ•¸é‡æˆ–æ¸›å°‘ç‰¹å¾µæ•¸é‡**\n"
                        warning_msg += "  - å»ºè­°æ¨£æœ¬æ•¸è‡³å°‘æ˜¯ç‰¹å¾µæ•¸çš„ 3-5 å€\n"
                    if diagnostics['has_constant_features']:
                        warning_msg += "- **ç«‹å³ç§»é™¤å¸¸æ•¸ç‰¹å¾µ**ï¼šé€™äº›ç‰¹å¾µå°æ¨¡å‹æ²’æœ‰è²¢ç»\n"
                        if diagnostics['constant_feature_names']:
                            warning_msg += f"  - éœ€è¦ç§»é™¤çš„ç‰¹å¾µï¼š{', '.join([f'`{name}`' for name in diagnostics['constant_feature_names']])}\n"
                    if diagnostics['has_multicollinearity']:
                        warning_msg += "- **è™•ç†å¤šé‡å…±ç·šæ€§**ï¼š\n"
                        if diagnostics['highly_correlated_pairs']:
                            warning_msg += "  - å¾é«˜åº¦ç›¸é—œçš„ç‰¹å¾µå°ä¸­ç§»é™¤å…¶ä¸­ä¸€å€‹ç‰¹å¾µ\n"
                        warning_msg += "  - æˆ–ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰é™ç¶­\n"
                        warning_msg += "  - æˆ–è€ƒæ…®ä½¿ç”¨æ­£å‰‡åŒ–æ–¹æ³•ï¼ˆRidge æˆ– Lasso å›æ­¸ï¼‰\n"
                    if diagnostics['condition_number'] is None or diagnostics['condition_number'] > 1e12:
                        warning_msg += "- **æª¢æŸ¥å®Œå…¨ç·šæ€§ç›¸é—œ**ï¼šæŸäº›ç‰¹å¾µå¯èƒ½æ˜¯å…¶ä»–ç‰¹å¾µçš„ç·šæ€§çµ„åˆ\n"
                    
                    st.warning(warning_msg)
                elif valid_p_count < total_count:
                    st.warning(f"âš ï¸ éƒ¨åˆ† p å€¼ç„¡æ³•è¨ˆç®—ï¼ˆ{valid_p_count}/{total_count} å€‹æˆåŠŸï¼‰ã€‚å¯èƒ½åŸå› ï¼š\n"
                              "- æŸäº›ç‰¹å¾µå­˜åœ¨å®Œå…¨ç·šæ€§ç›¸é—œ\n"
                              "- æ¨™æº–èª¤å·®ç‚º 0 æˆ–æ¥è¿‘ 0")
            
            # é¡¯ç¤ºå¸¶ p å€¼çš„ä¿‚æ•¸è¡¨æ ¼
            st.dataframe(params_df, use_container_width=True)
            
            # p å€¼èªªæ˜
            st.info(
                "ğŸ’¡ **p å€¼èªªæ˜**ï¼š\n\n"
                "- **p å€¼**ï¼šæª¢é©—ä¿‚æ•¸æ˜¯å¦é¡¯è‘—ä¸ç‚ºé›¶çš„çµ±è¨ˆæŒ‡æ¨™\n"
                "- **é¡¯è‘—æ€§æ¨™è¨˜**ï¼š\n"
                "  â€¢ `***`ï¼šp < 0.001ï¼ˆæ¥µé¡¯è‘—ï¼‰\n"
                "  â€¢ `**`ï¼šp < 0.01ï¼ˆéå¸¸é¡¯è‘—ï¼‰\n"
                "  â€¢ `*`ï¼šp < 0.05ï¼ˆé¡¯è‘—ï¼‰\n"
                "  â€¢ `.`ï¼šp < 0.1ï¼ˆé‚Šç·£é¡¯è‘—ï¼‰\n"
                "  â€¢ ç©ºç™½ï¼šp â‰¥ 0.1ï¼ˆä¸é¡¯è‘—ï¼‰\n\n"
                "- **è§£è®€**ï¼šp å€¼è¶Šå°ï¼Œè¡¨ç¤ºè©²ç‰¹å¾µå°ç›®æ¨™è®Šæ•¸çš„å½±éŸ¿è¶Šé¡¯è‘—\n"
                "- **ä¸€èˆ¬æ¨™æº–**ï¼šp < 0.05 é€šå¸¸èªç‚ºè©²ç‰¹å¾µå°ç›®æ¨™è®Šæ•¸æœ‰é¡¯è‘—å½±éŸ¿\n\n"
                "- **N/A èªªæ˜**ï¼šå¦‚æœ p å€¼é¡¯ç¤ºç‚º N/Aï¼Œå¯èƒ½æ˜¯å› ç‚ºï¼š\n"
                "  â€¢ ç‰¹å¾µä¹‹é–“å­˜åœ¨å®Œå…¨ç·šæ€§ç›¸é—œï¼ˆå¤šé‡å…±ç·šæ€§ï¼‰\n"
                "  â€¢ æ¨£æœ¬æ•¸é‡å¤ªå°‘æˆ–ç‰¹å¾µæ•¸é‡å¤ªå¤š\n"
                "  â€¢ çŸ©é™£ä¸å¯é€†ï¼Œç„¡æ³•è¨ˆç®—æ¨™æº–èª¤å·®"
            )
        except Exception as e:
            # å¦‚æœè¨ˆç®— p å€¼å¤±æ•—ï¼Œå›é€€åˆ°æ™®é€šé¡¯ç¤º
            params_df = display_model_parameters(model_info)
            st.dataframe(params_df, use_container_width=True)
            st.warning(f"âš ï¸ ç„¡æ³•è¨ˆç®— p å€¼ï¼š{str(e)}\n\n"
                      "å¯èƒ½åŸå› ï¼š\n"
                      "- ç‰¹å¾µä¹‹é–“å­˜åœ¨å®Œå…¨ç·šæ€§ç›¸é—œï¼ˆå¤šé‡å…±ç·šæ€§ï¼‰\n"
                      "- æ¨£æœ¬æ•¸é‡å¤ªå°‘æˆ–ç‰¹å¾µæ•¸é‡å¤ªå¤š\n"
                      "- è³‡æ–™åŒ…å«ç„¡æ•ˆå€¼ï¼ˆNaN æˆ– Infï¼‰")
    else:
        params_df = display_model_parameters(model_info)
        st.dataframe(params_df, use_container_width=True)
    
    # æ·»åŠ ä¿‚æ•¸è§£é‡‹èªªæ˜
    categorical_features_list = model.preprocessing_metadata.get('categorical_features', [])
    has_categorical = len(categorical_features_list) > 0
    
    if len(target_variables) == 1:
        explanation = (
            "ğŸ’¡ **ä¿‚æ•¸è§£é‡‹**ï¼š\n\n"
            "**æ•¸å€¼å‹ç‰¹å¾µä¿‚æ•¸**ï¼š\n"
            "- **ä¿‚æ•¸å€¼**ï¼šè¡¨ç¤ºè©²ç‰¹å¾µå°ç›®æ¨™è®Šæ•¸çš„å½±éŸ¿ç¨‹åº¦\n"
            "- **æ­£ä¿‚æ•¸**ï¼šç‰¹å¾µå€¼å¢åŠ  1 å–®ä½æ™‚ï¼Œç›®æ¨™è®Šæ•¸æœƒå¢åŠ ï¼ˆä¿‚æ•¸å€¼ï¼‰å–®ä½\n"
            "- **è² ä¿‚æ•¸**ï¼šç‰¹å¾µå€¼å¢åŠ  1 å–®ä½æ™‚ï¼Œç›®æ¨™è®Šæ•¸æœƒæ¸›å°‘ï¼ˆä¿‚æ•¸çµ•å°å€¼ï¼‰å–®ä½\n"
            "- **ä¿‚æ•¸çµ•å°å€¼å¤§å°**ï¼šè¡¨ç¤ºå½±éŸ¿çš„å¼·åº¦\n\n"
        )
        
        if has_categorical:
            explanation += (
                "**é¡åˆ¥å‹ç‰¹å¾µä¿‚æ•¸**ï¼ˆç¶“éç¨ç†±ç·¨ç¢¼ï¼‰ï¼š\n"
                "- é¡åˆ¥ç‰¹å¾µç¶“éç¨ç†±ç·¨ç¢¼å¾Œï¼Œæ¯å€‹é¡åˆ¥å€¼è®Šæˆä¸€å€‹äºŒé€²åˆ¶ç‰¹å¾µï¼ˆ0 æˆ– 1ï¼‰\n"
                "- **ä¿‚æ•¸å€¼**ï¼šè¡¨ç¤ºç›¸å°æ–¼åŸºæº–é¡åˆ¥ï¼ˆç¬¬ä¸€å€‹é¡åˆ¥ï¼‰ï¼Œè©²é¡åˆ¥å°ç›®æ¨™è®Šæ•¸çš„å½±éŸ¿\n"
                "- **æ­£ä¿‚æ•¸**ï¼šè©²é¡åˆ¥æœƒä½¿ç›®æ¨™è®Šæ•¸å¢åŠ ï¼ˆä¿‚æ•¸å€¼ï¼‰å–®ä½ï¼Œç›¸å°æ–¼åŸºæº–é¡åˆ¥\n"
                "- **è² ä¿‚æ•¸**ï¼šè©²é¡åˆ¥æœƒä½¿ç›®æ¨™è®Šæ•¸æ¸›å°‘ï¼ˆä¿‚æ•¸çµ•å°å€¼ï¼‰å–®ä½ï¼Œç›¸å°æ–¼åŸºæº–é¡åˆ¥\n"
                "- **ç¯„ä¾‹**ï¼šå¦‚æœã€Œé¡è‰²ã€ç‰¹å¾µæœ‰ã€Œç´…ã€è—ã€ç¶ ã€ä¸‰å€‹å€¼ï¼Œä¸”ã€Œç´…ã€æ˜¯åŸºæº–é¡åˆ¥ï¼š\n"
                "  â€¢ é¡è‰²_è—çš„ä¿‚æ•¸ = 0.5 â†’ ç›¸å°æ–¼ç´…è‰²ï¼Œè—è‰²æœƒä½¿ç›®æ¨™è®Šæ•¸å¢åŠ  0.5\n"
                "  â€¢ é¡è‰²_ç¶ çš„ä¿‚æ•¸ = -0.3 â†’ ç›¸å°æ–¼ç´…è‰²ï¼Œç¶ è‰²æœƒä½¿ç›®æ¨™è®Šæ•¸æ¸›å°‘ 0.3\n\n"
            )
        
        explanation += (
            "**æˆªè·**ï¼šç•¶æ‰€æœ‰æ•¸å€¼å‹ç‰¹å¾µç‚º 0ï¼Œä¸”æ‰€æœ‰é¡åˆ¥å‹ç‰¹å¾µç‚ºåŸºæº–é¡åˆ¥æ™‚çš„é æ¸¬å€¼"
        )
        
        st.info(explanation)
    else:
        explanation = (
            "ğŸ’¡ **å¤šç›®æ¨™è®Šæ•¸ä¿‚æ•¸è§£é‡‹**ï¼š\n\n"
            "- æ¯å€‹ç›®æ¨™è®Šæ•¸éƒ½æœ‰ç¨ç«‹çš„ä¿‚æ•¸å’Œæˆªè·\n\n"
            "**æ•¸å€¼å‹ç‰¹å¾µä¿‚æ•¸**ï¼š\n"
            "- **ä¿‚æ•¸å€¼**ï¼šè¡¨ç¤ºè©²ç‰¹å¾µå°å°æ‡‰ç›®æ¨™è®Šæ•¸çš„å½±éŸ¿ç¨‹åº¦\n"
            "- **æ­£ä¿‚æ•¸**ï¼šç‰¹å¾µå€¼å¢åŠ æ™‚ï¼Œç›®æ¨™è®Šæ•¸æœƒå¢åŠ \n"
            "- **è² ä¿‚æ•¸**ï¼šç‰¹å¾µå€¼å¢åŠ æ™‚ï¼Œç›®æ¨™è®Šæ•¸æœƒæ¸›å°‘\n\n"
        )
        
        if has_categorical:
            explanation += (
                "**é¡åˆ¥å‹ç‰¹å¾µä¿‚æ•¸**ï¼ˆç¶“éç¨ç†±ç·¨ç¢¼ï¼‰ï¼š\n"
                "- é¡åˆ¥ç‰¹å¾µç¶“éç¨ç†±ç·¨ç¢¼å¾Œï¼Œæ¯å€‹é¡åˆ¥å€¼è®Šæˆä¸€å€‹äºŒé€²åˆ¶ç‰¹å¾µ\n"
                "- **ä¿‚æ•¸å€¼**ï¼šè¡¨ç¤ºç›¸å°æ–¼åŸºæº–é¡åˆ¥ï¼ˆç¬¬ä¸€å€‹é¡åˆ¥ï¼‰ï¼Œè©²é¡åˆ¥å°ç›®æ¨™è®Šæ•¸çš„å½±éŸ¿\n"
                "- æ¯å€‹ç›®æ¨™è®Šæ•¸éƒ½æœ‰ç¨ç«‹çš„é¡åˆ¥ä¿‚æ•¸\n\n"
            )
        
        explanation += (
            "**æˆªè·**ï¼šç•¶æ‰€æœ‰ç‰¹å¾µç‚º 0 æ™‚è©²ç›®æ¨™è®Šæ•¸çš„é æ¸¬å€¼"
        )
        
        st.info(explanation)
    
    # å¦‚æœæœ‰é¡åˆ¥ç‰¹å¾µï¼Œé¡¯ç¤ºå“ªäº›ç‰¹å¾µæ˜¯é¡åˆ¥ç‰¹å¾µç·¨ç¢¼å¾Œçš„çµæœ
    if has_categorical:
        st.markdown("##### ğŸ“‹ é¡åˆ¥å‹ç‰¹å¾µèªªæ˜")
        encoded_feature_names = model.preprocessing_metadata.get('encoded_feature_names', [])
        
        # å˜—è©¦å¾ encoder ç²å–åŸºæº–é¡åˆ¥è³‡è¨Š
        encoder = model.encoder if hasattr(model, 'encoder') else None
        base_categories = {}
        
        if encoder is not None and hasattr(encoder, 'categories_'):
            for i, orig_feat in enumerate(categorical_features_list):
                if i < len(encoder.categories_):
                    categories = encoder.categories_[i]
                    if len(categories) > 0:
                        base_categories[orig_feat] = str(categories[0])
        
        # æŒ‰åŸå§‹é¡åˆ¥ç‰¹å¾µåˆ†çµ„
        categorical_groups = {}
        for encoded_name in encoded_feature_names:
            # ç·¨ç¢¼å¾Œçš„ç‰¹å¾µåç¨±æ ¼å¼ï¼šåŸå§‹ç‰¹å¾µå_é¡åˆ¥å€¼
            for orig_feat in categorical_features_list:
                if encoded_name.startswith(f"{orig_feat}_"):
                    if orig_feat not in categorical_groups:
                        categorical_groups[orig_feat] = []
                    categorical_groups[orig_feat].append(encoded_name)
                    break
        
        for orig_feat, encoded_features in categorical_groups.items():
            base_cat = base_categories.get(orig_feat, "ç¬¬ä¸€å€‹é¡åˆ¥å€¼")
            st.write(f"**{orig_feat}**ï¼ˆé¡åˆ¥å‹ç‰¹å¾µï¼‰ï¼š")
            st.write(f"  - ç·¨ç¢¼å¾Œçš„ç‰¹å¾µï¼š{', '.join(encoded_features)}")
            st.write(f"  - åŸºæº–é¡åˆ¥ï¼š**{base_cat}**ï¼ˆåœ¨ä¿‚æ•¸è¡¨æ ¼ä¸­ä¸é¡¯ç¤ºï¼Œä¿‚æ•¸ç‚º 0ï¼‰")
            st.write(f"  - å…¶ä»–é¡åˆ¥çš„ä¿‚æ•¸è¡¨ç¤ºç›¸å°æ–¼åŸºæº–é¡åˆ¥ã€Œ{base_cat}ã€çš„å½±éŸ¿")
            st.write("")
    
    # é¡¯ç¤ºä¿‚æ•¸çµ±è¨ˆè³‡è¨Šï¼ˆåƒ…å–®ä¸€ç›®æ¨™è®Šæ•¸æ™‚ï¼‰
    if not model_info.get('is_multi_output', False) and len(params_df) > 1:
        st.markdown("##### ğŸ“Š ä¿‚æ•¸çµ±è¨ˆè³‡è¨Š")
        coef_values = []
        for idx, row in params_df.iterrows():
            if row['ç‰¹å¾µ'] != 'æˆªè·':
                try:
                    coef_val = float(row['ä¿‚æ•¸'])
                    coef_values.append(coef_val)
                except:
                    pass
        
        if coef_values:
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("æœ€å¤§ä¿‚æ•¸", f"{max(coef_values, key=abs):.6f}")
            with col2:
                st.metric("æœ€å°ä¿‚æ•¸", f"{min(coef_values, key=abs):.6f}")
            with col3:
                st.metric("å¹³å‡çµ•å°å€¼", f"{np.mean([abs(x) for x in coef_values]):.6f}")
            with col4:
                st.metric("ä¿‚æ•¸ç¸½æ•¸", len(coef_values))
            
            # é¡¯ç¤ºå½±éŸ¿æœ€å¤§çš„å‰ 5 å€‹ç‰¹å¾µ
            coef_with_features = []
            coef_idx = 0
            for idx, row in params_df.iterrows():
                if row['ç‰¹å¾µ'] != 'æˆªè·':
                    try:
                        coef_val = float(row['ä¿‚æ•¸'])
                        coef_with_features.append((abs(coef_val), coef_val, row['ç‰¹å¾µ']))
                    except:
                        pass
            
            coef_with_features.sort(reverse=True, key=lambda x: x[0])
            top_features = coef_with_features[:min(5, len(coef_with_features))]
            
            if top_features:
                st.markdown("##### ğŸ” å½±éŸ¿æœ€å¤§çš„å‰ 5 å€‹ç‰¹å¾µï¼ˆæŒ‰ä¿‚æ•¸çµ•å°å€¼ï¼‰")
                top_df = pd.DataFrame({
                    'æ’å': range(1, len(top_features) + 1),
                    'ç‰¹å¾µ': [feat for _, _, feat in top_features],
                    'ä¿‚æ•¸å€¼': [f"{coef_val:.6f}" for _, coef_val, _ in top_features],
                    'ä¿‚æ•¸çµ•å°å€¼': [f"{abs_val:.6f}" for abs_val, _, _ in top_features]
                })
                st.dataframe(top_df, use_container_width=True)
    
    # è©•ä¼°æŒ‡æ¨™
    if split_data and X_test is not None and y_test is not None:
        # å¦‚æœæœ‰æ¸¬è©¦é›†ï¼Œé¡¯ç¤ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„è©•ä¼°æŒ‡æ¨™
        st.markdown("#### ğŸ“Š è©•ä¼°æŒ‡æ¨™")
        
        # è¨“ç·´é›†è©•ä¼°ï¼ˆå§‹çµ‚ä½¿ç”¨å–®æ¬¡è©•ä¼°ï¼‰
        st.markdown("##### ğŸ¯ è¨“ç·´é›†è©•ä¼°æŒ‡æ¨™")
        y_train_pred = model.predict(X_train)
        train_metrics_df = display_evaluation_metrics(y_train, y_train_pred)
        st.dataframe(train_metrics_df, use_container_width=True)
        
        # æ¸¬è©¦é›†è©•ä¼°ï¼ˆæ ¹æ“šé¸æ“‡çš„è©•ä¼°æ–¹å¼ï¼‰
        st.markdown("##### ğŸ¯ æ¸¬è©¦é›†è©•ä¼°æŒ‡æ¨™")
        
        if evaluation_method == "repeated":
            # é‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°
            from utils.evaluator import repeated_random_split_evaluate, format_metrics_with_std
            
            # æº–å‚™æ¨¡å‹åƒæ•¸
            model_params = {}
            if algorithm == "ç·šæ€§å›æ­¸":
                model_class = LinearRegressionModel
                # æ·»åŠ æ­£å‰‡åŒ–åƒæ•¸ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                regularization = None
                if regularization_type == "L1 (Lasso)":
                    regularization = 'l1'
                elif regularization_type == "L2 (Ridge)":
                    regularization = 'l2'
                if regularization:
                    model_params = {
                        'regularization': regularization,
                        'alpha': alpha
                    }
            else:  # æ¢¯åº¦ä¸‹é™
                model_class = GradientDescentModel
                model_params = {
                    'loss': loss_function,
                    'learning_rate': learning_rate,
                    'max_iter': max_iter,
                    'tol': tol,
                    'use_scaling': use_scaling
                }
            
            # åŸ·è¡Œé‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°
            with st.spinner(f"æ­£åœ¨åŸ·è¡Œé‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°ï¼ˆ{n_repeats} æ¬¡ï¼‰..."):
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                def progress_callback(current, total):
                    progress_bar.progress(current / total)
                    status_text.text(f"é€²åº¦ï¼š{current}/{total} æ¬¡è©•ä¼°å®Œæˆ")
                
                try:
                    # ä½¿ç”¨åŸå§‹è³‡æ–™ï¼ˆæœªæ“´å¢ã€æœªåˆ†å‰²ï¼‰é€²è¡Œè©•ä¼°
                    # æ³¨æ„ï¼šé€™è£¡éœ€è¦ä½¿ç”¨åŸå§‹ X å’Œ yï¼Œè€Œä¸æ˜¯æ“´å¢å¾Œçš„
                    # å¦‚æœ X å’Œ y ä¸åœ¨ä½œç”¨åŸŸå…§ï¼Œä½¿ç”¨ X_train + X_test å’Œ y_train + y_test åˆä½µ
                    if 'X' not in locals() or 'y' not in locals():
                        # å¦‚æœ X å’Œ y ä¸åœ¨ä½œç”¨åŸŸå…§ï¼Œå¾è¨“ç·´é›†å’Œæ¸¬è©¦é›†åˆä½µ
                        if X_test is not None and y_test is not None:
                            X_eval = pd.concat([X_train, X_test], ignore_index=True)
                            y_eval = pd.concat([y_train, y_test], ignore_index=True)
                        else:
                            # å¦‚æœæ²’æœ‰æ¸¬è©¦é›†ï¼Œä½¿ç”¨è¨“ç·´é›†
                            X_eval = X_train.copy()
                            y_eval = y_train.copy()
                    else:
                        # ä½¿ç”¨åŸå§‹ X å’Œ y
                        X_eval = X.copy()
                        y_eval = y.copy()
                    
                    # å–å¾—é¡åˆ¥å‹ç‰¹å¾µåˆ—è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    categorical_features_for_eval = st.session_state.get('categorical_features', None)
                    if categorical_features_for_eval:
                        categorical_features_for_eval = [f for f in categorical_features_for_eval if f in X_eval.columns]
                    
                    # æº–å‚™æ“´å¢åƒæ•¸ï¼ˆå¦‚æœå•Ÿç”¨æ“´å¢ï¼‰
                    aug_params = None
                    if enable_augmentation:
                        aug_params = {
                            'noise_type': augmentation_params['noise_type'],
                            'noise_strength': augmentation_params['noise_strength'],
                            'multiplier': augmentation_params['multiplier']
                        }
                    
                    mean_metrics, std_metrics, all_results = repeated_random_split_evaluate(
                        model_class,
                        model_params,
                        X_eval,  # ä½¿ç”¨åŸå§‹è³‡æ–™
                        y_eval,  # ä½¿ç”¨åŸå§‹è³‡æ–™
                        test_size=test_size,
                        n_repeats=n_repeats,
                        base_random_state=42,
                        progress_callback=progress_callback,
                        categorical_features=categorical_features_for_eval,
                        augmentation_params=aug_params  # å‚³éæ“´å¢åƒæ•¸
                    )
                    
                    progress_bar.empty()
                    status_text.empty()
                    
                    # é¡¯ç¤ºè©•ä¼°çµæœï¼ˆå¹³å‡å€¼ Â± æ¨™æº–å·®ï¼‰
                    st.success(f"âœ… é‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°å®Œæˆï¼ˆ{n_repeats} æ¬¡ï¼‰")
                    
                    # æ ¼å¼åŒ–ä¸¦é¡¯ç¤ºçµæœ
                    results_df = format_metrics_with_std(mean_metrics, std_metrics)
                    st.dataframe(results_df, use_container_width=True)
                    
                    # é¡¯ç¤º RÂ² æ¯”è¼ƒ
                    from sklearn.metrics import r2_score
                    train_r2 = r2_score(y_train, y_train_pred)
                    test_r2_mean = mean_metrics['RÂ²']
                    test_r2_std = std_metrics['RÂ²']
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("è¨“ç·´é›† RÂ²", f"{train_r2:.4f}")
                    with col2:
                        st.metric("æ¸¬è©¦é›† RÂ²ï¼ˆå¹³å‡å€¼ Â± æ¨™æº–å·®ï¼‰", f"{test_r2_mean:.4f} Â± {test_r2_std:.4f}")
                    
                    # é¡¯ç¤ºæ‰€æœ‰è©•ä¼°çµæœï¼ˆé è¨­æ‘ºç–Šï¼‰
                    with st.expander(f"ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰è©•ä¼°çµæœï¼ˆ{n_repeats} æ¬¡è©•ä¼°çš„è©³ç´°è¨˜éŒ„ï¼‰", expanded=False):
                        st.markdown("**æ¯æ¬¡è©•ä¼°çš„è©³ç´°çµæœï¼š**")
                        detailed_df = pd.DataFrame(all_results)
                        # æ ¼å¼åŒ–æ•¸å€¼é¡¯ç¤º
                        for col in detailed_df.columns:
                            if col not in ['é‡è¤‡æ¬¡æ•¸', 'éš¨æ©Ÿç¨®å­', 'è¨“ç·´é›†æ¨£æœ¬æ•¸', 'æ¸¬è©¦é›†æ¨£æœ¬æ•¸']:
                                detailed_df[col] = detailed_df[col].apply(lambda x: f"{x:.6f}")
                        st.dataframe(detailed_df, use_container_width=True)
                        
                        # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
                        st.markdown("**çµ±è¨ˆæ‘˜è¦ï¼š**")
                        summary_data = {
                            'æŒ‡æ¨™': ['è¨“ç·´é›† RÂ²', 'æ¸¬è©¦é›† RÂ²', 'è¨“ç·´é›† MSE', 'æ¸¬è©¦é›† MSE', 'è¨“ç·´é›† MAE', 'æ¸¬è©¦é›† MAE'],
                            'å¹³å‡å€¼': [
                                np.mean([r['è¨“ç·´é›† RÂ²'] for r in all_results]),
                                np.mean([r['æ¸¬è©¦é›† RÂ²'] for r in all_results]),
                                np.mean([r['è¨“ç·´é›† MSE'] for r in all_results]),
                                np.mean([r['æ¸¬è©¦é›† MSE'] for r in all_results]),
                                np.mean([r['è¨“ç·´é›† MAE'] for r in all_results]),
                                np.mean([r['æ¸¬è©¦é›† MAE'] for r in all_results])
                            ],
                            'æ¨™æº–å·®': [
                                np.std([r['è¨“ç·´é›† RÂ²'] for r in all_results]),
                                np.std([r['æ¸¬è©¦é›† RÂ²'] for r in all_results]),
                                np.std([r['è¨“ç·´é›† MSE'] for r in all_results]),
                                np.std([r['æ¸¬è©¦é›† MSE'] for r in all_results]),
                                np.std([r['è¨“ç·´é›† MAE'] for r in all_results]),
                                np.std([r['æ¸¬è©¦é›† MAE'] for r in all_results])
                            ],
                            'æœ€å°å€¼': [
                                np.min([r['è¨“ç·´é›† RÂ²'] for r in all_results]),
                                np.min([r['æ¸¬è©¦é›† RÂ²'] for r in all_results]),
                                np.min([r['è¨“ç·´é›† MSE'] for r in all_results]),
                                np.min([r['æ¸¬è©¦é›† MSE'] for r in all_results]),
                                np.min([r['è¨“ç·´é›† MAE'] for r in all_results]),
                                np.min([r['æ¸¬è©¦é›† MAE'] for r in all_results])
                            ],
                            'æœ€å¤§å€¼': [
                                np.max([r['è¨“ç·´é›† RÂ²'] for r in all_results]),
                                np.max([r['æ¸¬è©¦é›† RÂ²'] for r in all_results]),
                                np.max([r['è¨“ç·´é›† MSE'] for r in all_results]),
                                np.max([r['æ¸¬è©¦é›† MSE'] for r in all_results]),
                                np.max([r['è¨“ç·´é›† MAE'] for r in all_results]),
                                np.max([r['æ¸¬è©¦é›† MAE'] for r in all_results])
                            ]
                        }
                        summary_df = pd.DataFrame(summary_data)
                        # æ ¼å¼åŒ–æ•¸å€¼é¡¯ç¤º
                        for col in ['å¹³å‡å€¼', 'æ¨™æº–å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼']:
                            summary_df[col] = summary_df[col].apply(lambda x: f"{x:.6f}")
                        st.dataframe(summary_df, use_container_width=True)
                    
                    # å¦‚æœæ¸¬è©¦é›† RÂ² æ˜é¡¯ä½æ–¼è¨“ç·´é›† RÂ²ï¼Œæç¤ºå¯èƒ½éæ“¬åˆ
                    if test_r2_mean < train_r2 - 0.1:
                        st.warning("âš ï¸ æ¸¬è©¦é›† RÂ² æ˜é¡¯ä½æ–¼è¨“ç·´é›† RÂ²ï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆï¼ˆOverfittingï¼‰å•é¡Œã€‚å»ºè­°ï¼šæ¸›å°‘ç‰¹å¾µæ•¸é‡ã€å¢åŠ è¨“ç·´è³‡æ–™ã€æˆ–ä½¿ç”¨æ­£å‰‡åŒ–ã€‚")
                
                except Exception as e:
                    progress_bar.empty()
                    status_text.empty()
                    st.error(f"âŒ é‡è¤‡éš¨æ©Ÿåˆ†å‰²è©•ä¼°å¤±æ•—ï¼š{str(e)}\n\nå°‡ä½¿ç”¨å–®æ¬¡è©•ä¼°çµæœã€‚")
                    # å›é€€åˆ°å–®æ¬¡è©•ä¼°
                    y_test_pred = model.predict(X_test)
                    test_metrics_df = display_evaluation_metrics(y_test, y_test_pred)
                    st.dataframe(test_metrics_df, use_container_width=True)
                    
                    from sklearn.metrics import r2_score
                    train_r2 = r2_score(y_train, y_train_pred)
                    test_r2 = r2_score(y_test, y_test_pred)
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("è¨“ç·´é›† RÂ²", f"{train_r2:.4f}")
                    with col2:
                        st.metric("æ¸¬è©¦é›† RÂ²", f"{test_r2:.4f}")
                    
                    if test_r2 < train_r2 - 0.1:
                        st.warning("âš ï¸ æ¸¬è©¦é›† RÂ² æ˜é¡¯ä½æ–¼è¨“ç·´é›† RÂ²ï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆï¼ˆOverfittingï¼‰å•é¡Œã€‚å»ºè­°ï¼šæ¸›å°‘ç‰¹å¾µæ•¸é‡ã€å¢åŠ è¨“ç·´è³‡æ–™ã€æˆ–ä½¿ç”¨æ­£å‰‡åŒ–ã€‚")
        else:
            # å–®æ¬¡è©•ä¼°ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            y_test_pred = model.predict(X_test)
            test_metrics_df = display_evaluation_metrics(y_test, y_test_pred)
            st.dataframe(test_metrics_df, use_container_width=True)
            
            # æ¯”è¼ƒè¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„ RÂ²
            from sklearn.metrics import r2_score
            
            # è¨˜éŒ„å–®æ¬¡è©•ä¼°çµæœ
            single_eval_result = {
                'è©•ä¼°é¡å‹': 'å–®æ¬¡è©•ä¼°',
                'è¨“ç·´é›†æ¨£æœ¬æ•¸': len(X_train),
                'æ¸¬è©¦é›†æ¨£æœ¬æ•¸': len(X_test),
                'éš¨æ©Ÿç¨®å­': 42 if split_data else None
            }
            
            # å¤šç›®æ¨™è®Šæ•¸æ™‚ï¼Œè¨ˆç®—æ¯å€‹ç›®æ¨™çš„ RÂ² å’Œå¹³å‡ RÂ²
            if len(target_variables) > 1:
                train_r2_scores = [r2_score(y_train.iloc[:, i], y_train_pred.iloc[:, i]) 
                                  for i in range(len(target_variables))]
                test_r2_scores = [r2_score(y_test.iloc[:, i], y_test_pred.iloc[:, i]) 
                                 for i in range(len(target_variables))]
                train_r2 = np.mean(train_r2_scores)
                test_r2 = np.mean(test_r2_scores)
                
                # é¡¯ç¤ºæ¯å€‹ç›®æ¨™è®Šæ•¸çš„ RÂ²
                st.markdown("##### ğŸ“Š å„ç›®æ¨™è®Šæ•¸ RÂ² æ¯”è¼ƒ")
                r2_comparison_data = {
                    'ç›®æ¨™è®Šæ•¸': target_variables,
                    'è¨“ç·´é›† RÂ²': [f"{score:.4f}" for score in train_r2_scores],
                    'æ¸¬è©¦é›† RÂ²': [f"{score:.4f}" for score in test_r2_scores]
                }
                r2_comparison_df = pd.DataFrame(r2_comparison_data)
                st.dataframe(r2_comparison_df, use_container_width=True)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("è¨“ç·´é›†å¹³å‡ RÂ²", f"{train_r2:.4f}")
                with col2:
                    st.metric("æ¸¬è©¦é›†å¹³å‡ RÂ²", f"{test_r2:.4f}")
                
                # è¨˜éŒ„è©•ä¼°çµæœ
                single_eval_result.update({
                    'è¨“ç·´é›† RÂ²': train_r2,
                    'æ¸¬è©¦é›† RÂ²': test_r2
                })
            else:
                train_r2 = r2_score(y_train, y_train_pred)
                test_r2 = r2_score(y_test, y_test_pred)
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("è¨“ç·´é›† RÂ²", f"{train_r2:.4f}")
                with col2:
                    st.metric("æ¸¬è©¦é›† RÂ²", f"{test_r2:.4f}")
                
                # è¨˜éŒ„è©•ä¼°çµæœ
                single_eval_result.update({
                    'è¨“ç·´é›† RÂ²': train_r2,
                    'æ¸¬è©¦é›† RÂ²': test_r2
                })
            
            # è¨ˆç®—ä¸¦è¨˜éŒ„å…¶ä»–æŒ‡æ¨™
            from utils.evaluator import evaluate_model
            train_metrics = evaluate_model(y_train, y_train_pred)
            test_metrics = evaluate_model(y_test, y_test_pred)
            single_eval_result.update({
                'è¨“ç·´é›† MSE': train_metrics['MSE'],
                'æ¸¬è©¦é›† MSE': test_metrics['MSE'],
                'è¨“ç·´é›† MAE': train_metrics['MAE'],
                'æ¸¬è©¦é›† MAE': test_metrics['MAE']
            })
            
            # é¡¯ç¤ºæ‰€æœ‰è©•ä¼°çµæœï¼ˆé è¨­æ‘ºç–Šï¼‰
            with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´è©•ä¼°è¨˜éŒ„", expanded=False):
                st.markdown("**è©•ä¼°çµæœè¨˜éŒ„ï¼š**")
                eval_record_df = pd.DataFrame([single_eval_result])
                # æ ¼å¼åŒ–æ•¸å€¼é¡¯ç¤º
                for col in eval_record_df.columns:
                    if col not in ['è©•ä¼°é¡å‹', 'è¨“ç·´é›†æ¨£æœ¬æ•¸', 'æ¸¬è©¦é›†æ¨£æœ¬æ•¸'] and eval_record_df[col].dtype in [np.float64, np.float32]:
                        eval_record_df[col] = eval_record_df[col].apply(lambda x: f"{x:.6f}" if pd.notna(x) else "N/A")
                st.dataframe(eval_record_df, use_container_width=True)
                
                st.markdown("**è©³ç´°æŒ‡æ¨™ï¼š**")
                detailed_metrics = pd.DataFrame({
                    'æŒ‡æ¨™': ['RÂ²', 'MSE', 'MAE'],
                    'è¨“ç·´é›†': [
                        f"{train_metrics['RÂ²']:.6f}",
                        f"{train_metrics['MSE']:.6f}",
                        f"{train_metrics['MAE']:.6f}"
                    ],
                    'æ¸¬è©¦é›†': [
                        f"{test_metrics['RÂ²']:.6f}",
                        f"{test_metrics['MSE']:.6f}",
                        f"{test_metrics['MAE']:.6f}"
                    ]
                })
                st.dataframe(detailed_metrics, use_container_width=True)
                
            # å¦‚æœæ¸¬è©¦é›† RÂ² æ˜é¡¯ä½æ–¼è¨“ç·´é›† RÂ²ï¼Œæç¤ºå¯èƒ½éæ“¬åˆ
            if test_r2 < train_r2 - 0.1:
                st.warning("âš ï¸ æ¸¬è©¦é›† RÂ² æ˜é¡¯ä½æ–¼è¨“ç·´é›† RÂ²ï¼Œå¯èƒ½å­˜åœ¨éæ“¬åˆï¼ˆOverfittingï¼‰å•é¡Œã€‚å»ºè­°ï¼šæ¸›å°‘ç‰¹å¾µæ•¸é‡ã€å¢åŠ è¨“ç·´è³‡æ–™ã€æˆ–ä½¿ç”¨æ­£å‰‡åŒ–ã€‚")
    else:
        # æ²’æœ‰æ¸¬è©¦é›†ï¼Œåªé¡¯ç¤ºè¨“ç·´é›†è©•ä¼°æŒ‡æ¨™
        st.markdown("#### ğŸ“Š è©•ä¼°æŒ‡æ¨™ï¼ˆè¨“ç·´é›†ï¼‰")
        y_pred = model.predict(X_train)
        metrics_df = display_evaluation_metrics(y_train, y_pred)
        st.dataframe(metrics_df, use_container_width=True)
        
        # è¨˜éŒ„è©•ä¼°çµæœ
        from utils.evaluator import evaluate_model
        train_metrics = evaluate_model(y_train, y_pred)
        
        # é¡¯ç¤ºæ‰€æœ‰è©•ä¼°çµæœï¼ˆé è¨­æ‘ºç–Šï¼‰
        with st.expander("ğŸ“‹ æŸ¥çœ‹å®Œæ•´è©•ä¼°è¨˜éŒ„", expanded=False):
            st.markdown("**è©•ä¼°çµæœè¨˜éŒ„ï¼š**")
            eval_record = {
                'è©•ä¼°é¡å‹': 'è¨“ç·´é›†è©•ä¼°ï¼ˆç„¡æ¸¬è©¦é›†ï¼‰',
                'è¨“ç·´é›†æ¨£æœ¬æ•¸': len(X_train),
                'è¨“ç·´é›† RÂ²': train_metrics['RÂ²'],
                'è¨“ç·´é›† MSE': train_metrics['MSE'],
                'è¨“ç·´é›† MAE': train_metrics['MAE']
            }
            eval_record_df = pd.DataFrame([eval_record])
            # æ ¼å¼åŒ–æ•¸å€¼é¡¯ç¤º
            for col in eval_record_df.columns:
                if col not in ['è©•ä¼°é¡å‹', 'è¨“ç·´é›†æ¨£æœ¬æ•¸']:
                    eval_record_df[col] = eval_record_df[col].apply(lambda x: f"{x:.6f}")
            st.dataframe(eval_record_df, use_container_width=True)
            
            st.markdown("**è©³ç´°æŒ‡æ¨™ï¼š**")
            detailed_metrics = pd.DataFrame({
                'æŒ‡æ¨™': ['RÂ²', 'MSE', 'MAE'],
                'è¨“ç·´é›†': [
                    f"{train_metrics['RÂ²']:.6f}",
                    f"{train_metrics['MSE']:.6f}",
                    f"{train_metrics['MAE']:.6f}"
                ]
            })
            st.dataframe(detailed_metrics, use_container_width=True)
    
    # æ¨¡å‹ä¿å­˜
    st.markdown("---")
    st.markdown("### æ­¥é©Ÿ 10: ä¿å­˜æ¨¡å‹")
    
    # æ ¹æ“šç›®æ¨™è®Šæ•¸æ•¸é‡ç”Ÿæˆé è¨­æ¨¡å‹åç¨±
    if len(target_variables) == 1:
        default_model_name = f"{algorithm}_{target_variables[0]}"
    else:
        default_model_name = f"{algorithm}_{len(target_variables)}targets"
    
    model_name = st.text_input(
        "æ¨¡å‹åç¨±",
        value=default_model_name,
        help="è¼¸å…¥æ¨¡å‹åç¨±ï¼ˆä¸å«å‰¯æª”åï¼‰"
    )
    
    if st.button("ğŸ’¾ ä¿å­˜æ¨¡å‹", use_container_width=True):
        if model_name:
            try:
                filepath = save_model(model, model_name)
                st.success(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ï¼š{filepath}")
            except Exception as e:
                st.error(f"âŒ ä¿å­˜å¤±æ•—ï¼š{str(e)}")
        else:
            st.warning("è«‹è¼¸å…¥æ¨¡å‹åç¨±")
